{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "sys.path.append('/data/om/reflection_anydoor/datasets')\n",
    "sys.path.append('/data/om/reflection_anydoor')\n",
    "\n",
    "\n",
    "from mirrors import MirrorsDataset\n",
    "from cldm.logger import ImageLogger\n",
    "from cldm.model import create_model, load_state_dict\n",
    "from torch.utils.data import ConcatDataset\n",
    "from cldm.hack import disable_verbosity, enable_sliced_attention\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "save_memory = False\n",
    "disable_verbosity()\n",
    "if save_memory:\n",
    "    enable_sliced_attention()\n",
    "\n",
    "# Configs\n",
    "resume_path = '/data/om/reflection_anydoor/checkpoints/counterfactual_training_epoch=106-step=25572.ckpt'\n",
    "batch_size = 4\n",
    "logger_freq = 1000\n",
    "learning_rate = 1e-5\n",
    "sd_locked = False\n",
    "only_mid_control = False\n",
    "n_gpus = 1\n",
    "accumulate_grad_batches=1\n",
    "\n",
    "# First use cpu to load models. Pytorch Lightning will automatically move it to GPUs.\n",
    "model = create_model('./configs/anydoor.yaml').cpu()\n",
    "model.load_state_dict(load_state_dict(resume_path, location='cpu'))\n",
    "model.learning_rate = learning_rate\n",
    "model.sd_locked = sd_locked\n",
    "model.only_mid_control = only_mid_control\n",
    "\n",
    "# Datasets\n",
    "DConf = OmegaConf.load('./configs/datasets.yaml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MirrorsDataset(**DConf.Train.Mirrors)\n",
    "dataloader = DataLoader(dataset, num_workers=4, batch_size=batch_size, shuffle=True)\n",
    "logger = ImageLogger(batch_frequency=logger_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from datetime import datetime\n",
    "wandb.login(key='39e65ab86c39c92f1b18458c6cc56fee691e0705')\n",
    "wandb.init(project=\"anydoor_inpainting\")\n",
    "wandb.run.name = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "wandb_logger = WandbLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    gpus=[0], \n",
    "    precision=16, \n",
    "    accelerator=\"gpu\", \n",
    "    # callbacks=[logger], \n",
    "    progress_bar_refresh_rate=1, \n",
    "    accumulate_grad_batches=accumulate_grad_batches,\n",
    "    logger=wandb_logger,\n",
    "    )\n",
    "\n",
    "# Train!\n",
    "trainer.fit(model, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 64, 64])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "random_tensor = torch.rand(3, 64, 64)\n",
    "random_tensor.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anydoor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
